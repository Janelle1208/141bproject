{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JtrMU6hNz6y"
   },
   "source": [
    "\n",
    "<h1 style=\"font-family: Trebuchet MS; padding: 12px; font-size: 48px; color: #D97271; text-align: center; line-height: 1.25;\"><b>Breast Cancer<span style=\"color: #000000\"> Diagnosis Prediction</span></b><br><span style=\"color: #D97271; font-size: 24px\">with Various Machine Learning Models</span></h1>\n",
    "\n",
    "\n",
    "> **Notes**\n",
    "> \n",
    "> * **Colab Link**: [Click here](https://colab.research.google.com/drive/1ogt4xlQss13cZfJGgvUZsmu6jxl_9Yvv?usp=sharing)\n",
    "> * **Proposal**: [Latex](https://www.overleaf.com/9823217632srwkshctbrmd)\n",
    "> * **Report**: [Google doc](), [Latex](https://www.overleaf.com/4224522178csppcpwgcqst)\n",
    "> * **Presentation**: [Slides](https://docs.google.com/presentation/d/1VW1I-qKfIku8DwwTyGibQOvgtBOpEakpLM2pziOAptI/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D97271; color: #FFFFFF; padding: 12px; line-height: 1.5;\"> Dataset 1: Breast Cancers</div>\n",
    "\n",
    "<div class=\"warning\" style='background-color:#f5dada; color: #000000; border-left: solid #d68181 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em'>\n",
    "<b>Workflow:</b></p>\n",
    "<p style='margin-left:1em;'>\n",
    "\n",
    "1. Environment Setup\n",
    "2. Data Download\n",
    "3. Initial Data Explorations\n",
    "4. EDA (wait for graph edit)\n",
    "5. Feature Selections (wait for graph edit)\n",
    "6. Models\n",
    "\n",
    "</p>\n",
    "<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> <b></b> <i></i>\n",
    "</p></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryHXjPRTSdz3"
   },
   "source": [
    "##  <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\">Environment Setup</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:46:27.513009Z",
     "start_time": "2022-12-04T15:46:27.490801Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EX4nLRbSNxNV",
    "outputId": "b5dbda88-e4c2-4afd-a66e-1cacd8124e62",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required package - Usual packages\n",
    "# Color test\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    BOLD_RED_COLOR = '\\033[1m' + '\\033[91m'\n",
    "    BOLD_CYAN_COLOR = '\\033[1m' + '\\033[96m'\n",
    "    BOLD_GREEN_COLOR = '\\033[1m' + '\\033[92m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "print(color.BOLD_RED_COLOR + '\\nImporting all the required libraries...'+ color.END)\n",
    "\n",
    "## Basics\n",
    "import warnings \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "import pip\n",
    "import sys\n",
    "import os.path\n",
    "import platform\n",
    "import collections\n",
    "\n",
    "# Unusual packages Check and install\n",
    "packages = ['pywaffle', 'yellowbrick','missingno','ast','spacy','scispacy','httpx']\n",
    "for p in packages:\n",
    "    if not p in sys.modules:\n",
    "        pip.main(['install', p])\n",
    "\n",
    "## Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "from pywaffle import Waffle\n",
    "import missingno as msno\n",
    "\n",
    "# Models\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## SVC\n",
    "from sklearn.svm import SVC\n",
    "## ANN\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "# Model Visulization\n",
    "from yellowbrick.classifier import ConfusionMatrix, ROCAUC, PrecisionRecallCurve\n",
    "from yellowbrick.model_selection import LearningCurve, FeatureImportances\n",
    "from yellowbrick.contrib.wrapper import wrap\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "# Web Scrape\n",
    "import json\n",
    "import httpx\n",
    "import time\n",
    "import urllib\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "colors = ['#303030' ,'#98CDBE','#D97271','#FDF8F8']\n",
    "\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "mpl.rcParams['axes.spines.left'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = False\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# END\n",
    "print(color.BOLD_RED_COLOR + 'Finished\\n'+ color.END);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:18.340395Z",
     "start_time": "2022-12-04T15:02:18.239561Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "bc4OnQJAYute",
    "outputId": "f5ea2d0a-5984-406f-ca89-d89c303629bb"
   },
   "outputs": [],
   "source": [
    "sns.palplot(colors)\n",
    "plt.suptitle('Color Palette', fontweight='heavy', ha='center', fontsize=5, color=colors[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "DIdNdPwGR2o1"
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\">Jupyter Notebook Data Download </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:18.346836Z",
     "start_time": "2022-12-04T15:02:18.341692Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data.csv'):\n",
    "    mysystem = platform.system()\n",
    "    file_download_link = 'https://drive.google.com/uc?id=1mLS7-mhwMhkcBdt0NdBOHK3iz-EXzBeS'\n",
    "\n",
    "    if mysystem != 'Windows':\n",
    "        !wget -O cancer_data.zip --no-check-certificate \"$file_download_link\"\n",
    "        !unzip cancer_data.zip\n",
    "    else:\n",
    "        if not 'gdown' in sys.modules:\n",
    "            pip.main(['install', 'gdown'])\n",
    "            import gdown\n",
    "\n",
    "        file_id = '116MM-XRAVwAORT8iJXKnfA6Q0X-qZCjvi'\n",
    "        output = \"data.csv\"\n",
    "        gdown.download(\n",
    "            f\"https://drive.google.com/uc?export=download&confirm=pbef&id={file_id}\",\n",
    "            output\n",
    "        )\n",
    "    \n",
    "    print('Download Completed')\n",
    "else:\n",
    "    print('Found data files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "nYPW6l10C4y5"
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\">Initial Data Explorations </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:18.371827Z",
     "start_time": "2022-12-04T15:02:18.348862Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "hidden": true,
    "id": "kJpyqxAAO1yO",
    "outputId": "e59c15ab-9754-4f57-a7fd-dc5a722ff900"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.329688Z",
     "start_time": "2022-12-04T15:02:18.373019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "hidden": true,
    "id": "slIA4UqcQIfr",
    "outputId": "773d1dd0-ff33-4d4c-d6f8-4508e98ea632"
   },
   "outputs": [],
   "source": [
    "# Check null variables\n",
    "msno.bar(df)\n",
    "plt.title('Missing / Null Values Graph', fontweight='heavy', \n",
    "          ha='center', fontsize=25, color=colors[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "WLMlergcFMSy"
   },
   "source": [
    "We can see that the column `Unnamed: 32` has 569 missing values. Therefore we remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.333921Z",
     "start_time": "2022-12-04T15:02:21.330863Z"
    },
    "hidden": true,
    "id": "mhVCwq1cgOLI"
   },
   "outputs": [],
   "source": [
    "# Set new dataframe\n",
    "df = df.drop(df.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TqdC54YhRr_b"
   },
   "source": [
    "### Delete an unnecessary column: `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.337559Z",
     "start_time": "2022-12-04T15:02:21.334996Z"
    },
    "hidden": true,
    "id": "D_Ve6ebFR3d0"
   },
   "outputs": [],
   "source": [
    "# Final cleaned dataset\n",
    "df = df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.341462Z",
     "start_time": "2022-12-04T15:02:21.338599Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "M1MHOqRmGskS",
    "outputId": "39e41112-b256-4bf6-86d0-69d44351f283",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.350467Z",
     "start_time": "2022-12-04T15:02:21.342861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "OBzyk6PqIHG7",
    "outputId": "226fdd1f-6e1b-4a9c-a99f-eb6d955f0820"
   },
   "outputs": [],
   "source": [
    "# Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.394277Z",
     "start_time": "2022-12-04T15:02:21.353489Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "hidden": true,
    "id": "hqqbwXCYSaah",
    "outputId": "7712b639-2ef0-4b1b-e2f4-e400c2c2d69a"
   },
   "outputs": [],
   "source": [
    "# Description of dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7iAkpAiCQ8p"
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\"> EDA </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:21.794786Z",
     "start_time": "2022-12-04T15:02:21.395846Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "YD21SbC01Pgc",
    "outputId": "9c8e5fcf-fc8d-40ba-da72-561693316241",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Waffle plot of diagnosis distribution of the dataset\n",
    "target = df['diagnosis']\n",
    "targetDist = round(target.value_counts(normalize = True),2)*100\n",
    "labels = ['Benign', 'Malignant']\n",
    "\n",
    "# Main Figure\n",
    "fig = plt.figure(FigureClass=Waffle, rows=5, colors=colors[1:3], values=targetDist, figsize=(9, 5))\n",
    "# Title and subtitle\n",
    "plt.suptitle('Breast Cancer Diagnosis Distributions', fontweight='heavy', y=0.79, ha='center', \n",
    "             fontsize=13, color=colors[0]) \n",
    "plt.title('It appears that the dataset has more benign data.\\n', \n",
    "          style='italic', fontsize=8, loc='center', y=0.98, ha='center',  color=colors[0])\n",
    "\n",
    "# Legends\n",
    "plt.text(1.4, -0.25, '{0:.2f}%'.format(targetDist[0]), color=colors[1], \n",
    "         fontsize=16, ha='center', weight='bold', va='bottom')\n",
    "plt.text(1.4, -0.28, labels[0], color=colors[0], fontsize=10, ha='center', va='top', weight='bold')\n",
    "plt.text(3.43, -0.25, '{0:.2f}%'.format(targetDist[1]), color=colors[2], \n",
    "         fontsize=16, ha='center', weight='bold', va='bottom')\n",
    "plt.text(3.43, -0.28, labels[1], color=colors[0], fontsize=10, ha='center', va='top', weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:22.056671Z",
     "start_time": "2022-12-04T15:02:21.796107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pie chart\n",
    "fig = go.Pie(labels = ['Benign','Malignant'], values = df['diagnosis'].value_counts(), \n",
    "               textfont=dict(size=18), marker = dict(colors=[colors[2], colors[1]], \n",
    "                line=dict(color='#000000', width=2)))\n",
    "layout = dict(title = 'Breast Cancer Diagnosis Distributions')\n",
    "fig = dict(data = [fig], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:22.061628Z",
     "start_time": "2022-12-04T15:02:22.057883Z"
    }
   },
   "outputs": [],
   "source": [
    "def barplot1(start,end):\n",
    "    plt.figure(figsize = (20, 20))\n",
    "    sub_plot = 0\n",
    "\n",
    "    for column in df.columns[start:end]:\n",
    "        ax = plt.subplot(5, 3, sub_plot+1)\n",
    "        plot = sns.distplot(df[column])\n",
    "        plot.set(xticklabels=[])\n",
    "        plot.set(yticklabels=[])\n",
    "        plot.set(ylabel=None)\n",
    "        xlabel = ' '.join([w.capitalize() for w in column.split('_') ])\n",
    "        plot.set_xlabel(xlabel, fontsize = 15, fontweight='heavy')\n",
    "        sub_plot += 1\n",
    "    plt.suptitle(f'Distribution plot for col {start} to col {end-1}', \n",
    "                 fontweight='heavy',color=colors[0],fontsize=20) \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:27.035023Z",
     "start_time": "2022-12-04T15:02:22.062848Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "barplot1(1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:32.196813Z",
     "start_time": "2022-12-04T15:02:27.036932Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barplot1(16,31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO8WTT8LTpSp"
   },
   "source": [
    "Based the description of the dataset and distribution plots for each features, we can find that all the predictors are on a very different scale. Therefore before doing further data analysis in each features, we decide to standardize our dataset making the variables more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:32.206280Z",
     "start_time": "2022-12-04T15:02:32.198162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standarization of dataset\n",
    "df_S = df.copy()\n",
    "df_S[df_S.columns[1:]] = preprocessing.scale(df[df.columns[1:]])\n",
    "features = df_S['diagnosis']\n",
    "data = df_S[df_S.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:32.211672Z",
     "start_time": "2022-12-04T15:02:32.207410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "def cancer_boxplot(start,end,label=''):\n",
    "    bp = pd.concat([features,data.iloc[:,start:end]],axis=1)\n",
    "    bp = pd.melt(bp, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18, 8)\n",
    "    plot = sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=bp,\n",
    "                palette=[colors[2],colors[1]],width=0.4)\n",
    "    # Adjust Graph\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.suptitle(f'Descriptive Box Plot by Diagnosis Groups ({label})', \n",
    "                 fontweight='heavy', ha='center',y=0.93, \n",
    "             fontsize=18, color=colors[0])\n",
    "    plt.grid(axis='y')\n",
    "    plot.set(xlabel=None)\n",
    "    plot.set(ylabel=None)\n",
    "    plt.legend(title='Diagnosis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:33.532694Z",
     "start_time": "2022-12-04T15:02:32.212903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cancer_boxplot(0,10,'plot 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:34.792790Z",
     "start_time": "2022-12-04T15:02:33.534321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cancer_boxplot(10,20,'plot 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:36.257590Z",
     "start_time": "2022-12-04T15:02:34.794073Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cancer_boxplot(20,31,'plot 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:41.553480Z",
     "start_time": "2022-12-04T15:02:36.258706Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Violinplot\n",
    "df_S[\"all\"] = \"\"\n",
    "fig = plt.figure(figsize = (15, 25))\n",
    "sub_plot = 0\n",
    "\n",
    "for col in data.columns:\n",
    "    plt.subplot(6, 5, sub_plot+1)\n",
    "    \n",
    "    plot=sns.violinplot(x='all', y=col, hue=\"diagnosis\", data=df_S,palette=[colors[2],colors[1]], \n",
    "                        split=True, edgecolor = 'solidblack',linewidth = 2,inner=\"quart\")\n",
    "    plt.legend([],[], frameon=False)\n",
    "    plot.set(xticklabels=[])\n",
    "    plot.set(yticklabels=[])\n",
    "    plot.set(ylabel=None)\n",
    "    xlabel = ' '.join([w.capitalize() for w in col.split('_') ])\n",
    "    plot.set_xlabel(xlabel, fontsize = 15, fontweight='heavy')\n",
    "    sub_plot += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Title\n",
    "plt.suptitle('Breast Cancer Diagnosis Distributions', fontweight='heavy', \n",
    "             y=1.04,ha='center', fontsize=21, color=colors[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The distribution between malignant and benign tumors among all the cell image data shows that in most of the case the malignant tumors are more likely to have larger and faster-growing than original cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since there are lots of features in the dataset, we need to delete some variables. \n",
    "The first step we should do is to use heatmap to see the correlation among each columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:41.626695Z",
     "start_time": "2022-12-04T15:02:41.554963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Interactive heatmap\n",
    "df_corr_round = df.corr().round(2)\n",
    "fig = px.imshow(df_corr_round, text_auto=True,color_continuous_scale='Viridis')\n",
    "fig.layout.height = 1000\n",
    "fig.layout.width = 1000\n",
    "fig.update_layout(dragmode=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on the heatmap, we can see that some pairs of variables have high correlations, such as `radius_worst` and `perimeter_mean`, which means that there is multicollinearity in the dataset. To solve this, we will apply correlation filter in the latter steps. Since there are 30 features in our dataset, to avoid overfitting issue, we should use PCA and correlation filter to reduce the dimension of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T23:33:14.552916Z",
     "start_time": "2022-11-23T23:33:14.550816Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Multivariate Analysis\n",
    "#### Check the correlation between few features by pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T09:58:30.882318Z",
     "start_time": "2022-12-04T09:58:30.873850Z"
    },
    "hidden": true
   },
   "source": [
    "We first plot top 4 pairs where their correlations between features are positive and high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:41.632546Z",
     "start_time": "2022-12-04T15:02:41.627835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "palette = {'M': colors[2], 'B': colors[1]}\n",
    "\n",
    "def scatter_plot(lst,title):\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    sub_plot = 0\n",
    "\n",
    "    for cols in lst:\n",
    "        ax = plt.subplot(2, 2, sub_plot+1)\n",
    "        plot = sns.scatterplot(x = df[cols[0]], y = df[cols[1]], hue = 'diagnosis', \n",
    "                    data = df, palette = palette)\n",
    "        plot.patch.set_edgecolor('black')  \n",
    "        plot.patch.set_linewidth('1')\n",
    "        plt.legend([],[], frameon=False)\n",
    "        xlabel = ' '.join([w.capitalize() for w in cols[0].split('_') ])\n",
    "        ylabel = ' '.join([w.capitalize() for w in cols[1].split('_') ])\n",
    "        plot.set_xlabel(xlabel, fontweight='heavy')\n",
    "        plot.set_ylabel(ylabel, fontweight='heavy')\n",
    "        sub_plot += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add global legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles, labels,loc=(1.05, 2),title='Diagnosis')\n",
    "\n",
    "    # Add title\n",
    "    plt.suptitle(title, fontweight='heavy',y=1.04,ha='center', fontsize=15, color=colors[0]) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:41.636001Z",
     "start_time": "2022-12-04T15:02:41.633757Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "positive_corr = [['perimeter_mean','radius_worst'],\n",
    "['area_mean','radius_worst'],\n",
    "['area_worst','radius_worst'],\n",
    "['texture_mean','texture_worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:42.991878Z",
     "start_time": "2022-12-04T15:02:41.637087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scatter_plot(positive_corr,'Positive Correlated Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then plot top 4 pairs where their correlations between features are positive and low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:42.995626Z",
     "start_time": "2022-12-04T15:02:42.993288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "positive_uncorr = [['smoothness_mean','texture_mean'],\n",
    "['texture_worst','symmetry_mean'],\n",
    "['symmetry_se','texture_worst'],\n",
    "['radius_mean','fractal_dimension_worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:44.453922Z",
     "start_time": "2022-12-04T15:02:42.996912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scatter_plot(positive_uncorr,'Positive Uncorrelated Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then plot top 4 pairs where their correlations between features are negative and high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:44.462722Z",
     "start_time": "2022-12-04T15:02:44.460835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "negative_corr = [['area_mean','fractal_dimension_mean'],\n",
    "['smoothness_se','perimeter_mean'],\n",
    "['radius_mean','fractal_dimension_mean'],\n",
    "['area_mean','smoothness_se']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:45.879757Z",
     "start_time": "2022-12-04T15:02:44.463655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scatter_plot(negative_corr,'Negative Correlated Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that the cancer cell features which are highly correlated, which means that we have multicolinear features. Those features could affect the predictions in the latter model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\"> Feature Selections </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Binarize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:45.886804Z",
     "start_time": "2022-12-04T15:02:45.881129Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "df['diagnosis'] = lb.fit_transform(df['diagnosis'])\n",
    "# Check the distribution\n",
    "df['diagnosis'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Original Standardized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:45.894295Z",
     "start_time": "2022-12-04T15:02:45.887899Z"
    },
    "hidden": true,
    "id": "WE41olAISDHY"
   },
   "outputs": [],
   "source": [
    "# Split data 8:2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['diagnosis'], axis=1), \n",
    "                                                    df.diagnosis, test_size=0.2, random_state=14)\n",
    "\n",
    "# Standardization\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(color.BOLD_CYAN_COLOR + '\\nTraining data shape:'+ color.END, X_train.shape)\n",
    "print(color.BOLD_CYAN_COLOR + 'Testing data shape:'+ color.END, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "wyRFyK_tSBvv"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:45.911083Z",
     "start_time": "2022-12-04T15:02:45.895379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Screen plot of PCA\n",
    "# Performing the principal component analysis\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit_transform(X_train)\n",
    "percent_var = np.round(pca.explained_variance_ratio_*100, decimals=1)   \n",
    "labels = ['PC' + str(p) for p in range(1,len(percent_var)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:45.915123Z",
     "start_time": "2022-12-04T15:02:45.912454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PCA_df = pd.DataFrame(\n",
    "    {'labels': labels,\n",
    "     'percent_var': percent_var})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.538317Z",
     "start_time": "2022-12-04T15:02:45.916886Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "plot1 = sns.barplot(data=PCA_df, x=\"labels\", y=\"percent_var\",color=colors[1],ci=None)\n",
    "plot1.patch.set_edgecolor('black')  \n",
    "plot1.patch.set_linewidth('1')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Variation explained')\n",
    "plt.title(f'PCA Screen Plot First 7 Components ({percent_var.sum()}%)',fontweight='heavy')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "plot2 = sns.lineplot(data=PCA_df, x=\"labels\", y=\"percent_var\",marker='o',color=colors[0], ax=ax2)\n",
    "plot2.set(yticklabels=[])\n",
    "plot2.set(ylabel=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:43:09.183082Z",
     "start_time": "2022-12-04T13:43:09.175842Z"
    },
    "hidden": true
   },
   "source": [
    "Let us drop PCA plot with 2 and 3 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PCA Plot (n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.595121Z",
     "start_time": "2022-12-04T15:02:46.539683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(df[df.columns[1:]])\n",
    "fig = px.scatter(components, x=0, y=1, color=df['diagnosis'],\n",
    "                labels={'0': 'PC 1', '1': 'PC 2'})\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(marker=dict(size=10,line=dict(width=1.5,color=colors[0])),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PCA Plot (n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.652138Z",
     "start_time": "2022-12-04T15:02:46.596759Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(df[df.columns[1:]])\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['diagnosis'],\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'})\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(marker=dict(size=5,line=dict(width=2,color=colors[0])),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After applying PCA and calculating the percentage of explained variances (PEV) of the first ten components, the first 5 PCs can explain 85% of the variance. Therefore, we will focus on the first 5 PCs in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.661571Z",
     "start_time": "2022-12-04T15:02:46.653606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "\n",
    "print(color.BOLD_CYAN_COLOR + '\\nTraining data shape:'+ color.END, X_train_PCA.shape)\n",
    "print(color.BOLD_CYAN_COLOR + 'Testing data shape:'+ color.END, X_test_PCA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Feature Selections BASED ON CORRELATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on the heatmap above we can see that some columns are highly correlated to each others. \n",
    "\n",
    "We dropped a total of nine features: “area-mean, perimeter-mean, radius-worst, area-worst, perimeter-worst, texture-worst, concavity-mean, perimeter-se, area-se.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.670473Z",
     "start_time": "2022-12-04T15:02:46.663305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Correlations\n",
    "corr = df[df.columns[1:]].corr()\n",
    "corr = corr.abs().unstack().drop_duplicates()\n",
    "for index, value in corr.items():\n",
    "    if value != 1 and value >= 0.85:\n",
    "        col1 = ' '.join([w.capitalize() for w in index[0].split('_') ])\n",
    "        col2 = ' '.join([w.capitalize() for w in index[1] .split('_') ])\n",
    "        print(color.BOLD_CYAN_COLOR + f'\\n    {col1} and {col2}:'+ color.END,\n",
    "             round(value,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Based on the above list, we will drop:**\n",
    "1. `perimeter_mean`\n",
    "2. `area_mean`\n",
    "3. `radius_worst`\n",
    "4. `area_worst`\n",
    "5. `perimeter_worst`\n",
    "6. `texture_worst`\n",
    "7. `concavity_mean`\n",
    "8. `perimeter_se`\n",
    "9. `area_se`\n",
    "10. `compactness_worst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.675527Z",
     "start_time": "2022-12-04T15:02:46.672728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop list\n",
    "drop_lst = ['perimeter_mean','area_mean','radius_worst','area_worst',\n",
    "           'perimeter_worst','texture_worst','concavity_mean','perimeter_se',\n",
    "           'area_se','compactness_worst']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:02:46.706532Z",
     "start_time": "2022-12-04T15:02:46.677515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Interactive heatmap\n",
    "df1 = df.drop(drop_lst,axis = 1)\n",
    "df_corr_round = df1.corr().round(2)\n",
    "fig = px.imshow(df_corr_round, text_auto=True, color_continuous_scale='Viridis')\n",
    "fig.layout.height = 1000\n",
    "fig.layout.width = 1000\n",
    "fig.update_layout(dragmode=False)\n",
    "fig.update_xaxes(tickangle=90)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T15:03:00.230300Z",
     "start_time": "2022-12-04T15:03:00.226924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# corr datasets\n",
    "column_lst = list(df.columns[1:])\n",
    "index_lst = []\n",
    "for i in drop_lst:\n",
    "    index_lst.append(column_lst.index(i))\n",
    "\n",
    "X_train_CORR = np.delete(X_train, np.s_[index_lst], axis=1)\n",
    "X_text_CORR = np.delete(X_test, np.s_[index_lst], axis=1)\n",
    "\n",
    "print(color.BOLD_CYAN_COLOR + '\\nTraining data shape:'+ color.END, X_train_CORR.shape)\n",
    "print(color.BOLD_CYAN_COLOR + 'Testing data shape:'+ color.END, X_text_CORR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "D-JRvUXcSELf"
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5;border-left: solid #d68181 4px; border-radius: 5px\">Models</div>\n",
    "\n",
    "<div class=\"warning\" style='padding:0.1em; background-color:#f5dada; color:#3c695bb; border-top: solid #d68181 5px; border-radius: 3px; padding:1em;' >\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<p style='margin-left:1em;'>\n",
    "\n",
    "**We will use three datasets to train models:**\n",
    "\n",
    "1. Original: `X_train`, `X_text`\n",
    "2. After PCA: `X_train_PCA`, `X_text_PCA`\n",
    "3. After correlation filter: `X_train_CORR`, `X_text_CORR`\n",
    "</p>\n",
    "<p></p></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:13.728920Z",
     "start_time": "2022-12-03T08:23:13.726754Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Models dict\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:13.736923Z",
     "start_time": "2022-12-03T08:23:13.729955Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Performance Evaluation Function --- #\n",
    "# Confusion matrix, ROC AUC\n",
    "\n",
    "def dataset_type(data_type):\n",
    "    \"\"\" Wait for edit\n",
    "    \"\"\"\n",
    "    if data_type == 'PCA':\n",
    "        train_data = X_train_PCA\n",
    "        test_data = X_test_PCA\n",
    "    elif data_type == 'CORR':\n",
    "        train_data = X_train_CORR\n",
    "        test_data = X_text_CORR\n",
    "    elif data_type == 'ORI':\n",
    "        train_data = X_train\n",
    "        test_data = X_test\n",
    "    return (train_data,test_data)\n",
    "\n",
    "\n",
    "def evaluate_models(model_name, model_fun, data_type='ORI'):\n",
    "    \"\"\"Wait for edit\n",
    "    \n",
    "    * Has Code references  \n",
    "    \"\"\"\n",
    "    classifier = model_fun\n",
    "    train_data,test_data = dataset_type(data_type)\n",
    "    \n",
    "    print(color.BOLD_RED_COLOR + f'\\n{model_name} - {data_type}: '+ color.END)\n",
    "    \n",
    "    # Fitting models\n",
    "    print(color.BOLD_CYAN_COLOR + '\\n.:. Model Fittings .:. '+ color.END)\n",
    "    classifier.fit(train_data, y_train)\n",
    "    print(classifier)\n",
    "    y_pred = classifier.predict(test_data)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(color.BOLD_CYAN_COLOR + '\\n.:. Classification Report .:. '+ color.END)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(color.BOLD_CYAN_COLOR + '\\n.:. Models Evaluation Graphs .:. '+ color.END)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(color.BOLD_GREEN_COLOR + '      Accuracy: %f' % accuracy + color.END,'\\n')\n",
    "    \n",
    "    # Add accuracy to the models dict\n",
    "    models[model_name+'_'+data_type] = accuracy\n",
    "    \n",
    "    # Figure settings\n",
    "    set_palette('sns_pastel')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11,5))\n",
    "    \n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = ConfusionMatrix(classifier, ax=ax1, cmap='bugn', classes=['B', 'M'],\n",
    "                                  title=f'{model_name} Confusion Matrix')\n",
    "    conf_matrix.fit(train_data, y_train)\n",
    "    conf_matrix.score(test_data, y_test)\n",
    "    conf_matrix.finalize()\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        logrocauc = ROCAUC(classifier, classes=['B', 'M'], cmap='sns_pastel',\n",
    "                       ax=ax2, title=f'{model_name} ROC AUC')\n",
    "        logrocauc.fit(train_data, y_train)\n",
    "        logrocauc.score(test_data, y_test)\n",
    "        logrocauc.finalize()\n",
    "    except:\n",
    "        logrocauc = ROCAUC(classifier, classes=['B', 'M'], cmap='sns_pastel',\n",
    "                       ax=ax2, title=f'{model_name} ROC AUC',binary=True)\n",
    "        logrocauc.fit(train_data, y_train)\n",
    "        logrocauc.score(test_data, y_test)\n",
    "        logrocauc.finalize()\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:13.740282Z",
     "start_time": "2022-12-03T08:23:13.738059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# --- Logistic Models Function --- #\n",
    "def log_reg():    \n",
    "    classifier = LogisticRegression()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:14.404388Z",
     "start_time": "2022-12-03T08:23:13.741219Z"
    },
    "hidden": true,
    "id": "O5xkh6JcSGMP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logistic Modles - Original\n",
    "evaluate_models('Logistic Regression',log_reg());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.078753Z",
     "start_time": "2022-12-03T08:23:14.405583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Logistic Modles - PCA\n",
    "evaluate_models('Logistic Regression',log_reg(),'PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.743655Z",
     "start_time": "2022-12-03T08:23:15.079898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Logistic Models - Correlation Filter\n",
    "evaluate_models('Logistic Regression',log_reg(),'CORR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.746806Z",
     "start_time": "2022-12-03T08:23:15.744821Z"
    },
    "hidden": true,
    "id": "vBJn_9_dWzY5"
   },
   "outputs": [],
   "source": [
    "# --- KNN Models Function --- #\n",
    "def KNN_model():\n",
    "    classifier = KNeighborsClassifier()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.749799Z",
     "start_time": "2022-12-03T08:23:15.748060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "evaluate_models('KNN', KNN_model());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.752566Z",
     "start_time": "2022-12-03T08:23:15.750885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "evaluate_models('KNN', KNN_model(),'PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.755380Z",
     "start_time": "2022-12-03T08:23:15.753733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# High Correlation Filter\n",
    "evaluate_models('KNN', KNN_model(),'CORR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:15.758786Z",
     "start_time": "2022-12-03T08:23:15.756479Z"
    },
    "hidden": true,
    "id": "Iu73zO8RWz6O"
   },
   "outputs": [],
   "source": [
    "# --- SVM (linear) Models Function --- #\n",
    "def SVM_model_linear():\n",
    "    classifier = SVC(kernel='linear')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:16.396601Z",
     "start_time": "2022-12-03T08:23:15.759806Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "evaluate_models('SVM (Linear)',SVM_model_linear());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:17.068015Z",
     "start_time": "2022-12-03T08:23:16.397897Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "evaluate_models('SVM (Linear)', SVM_model_linear(),'PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:17.751296Z",
     "start_time": "2022-12-03T08:23:17.069168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# High Correlation Filter\n",
    "evaluate_models('SVM (Linear)', SVM_model_linear(),'CORR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:17.754988Z",
     "start_time": "2022-12-03T08:23:17.752673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# --- SVM (rbf) Models Function --- #\n",
    "def SVM_model_rbf():\n",
    "    classifier = SVC(kernel='rbf')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:18.409499Z",
     "start_time": "2022-12-03T08:23:17.756174Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "evaluate_models('SVM (rbf)',SVM_model_rbf());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.065656Z",
     "start_time": "2022-12-03T08:23:18.410830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "evaluate_models('SVM (rbf)',SVM_model_rbf(),'PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.713921Z",
     "start_time": "2022-12-03T08:23:19.066864Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# High Correlation Filter\n",
    "evaluate_models('SVM (rbf)',SVM_model_rbf(),'CORR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.717217Z",
     "start_time": "2022-12-03T08:23:19.715102Z"
    },
    "hidden": true,
    "id": "mUGWDfA-W3Iw"
   },
   "outputs": [],
   "source": [
    "# --- ANN Models Function --- #\n",
    "def ANN_model(data_type='ORI'):\n",
    "    train_data,test_data = dataset_type(data_type)\n",
    "\n",
    "    # Initialize ANN models\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(16, activation='relu', input_dim=train_data.shape[1]))\n",
    "    classifier.add(Dense(15, activation='relu'))\n",
    "    classifier.add(Dense(1, activation='sigmoid'))\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    classifier.fit(train_data, y_train, batch_size=10, epochs=100);\n",
    "    y_pred = classifier.predict(test_data)\n",
    "\n",
    "    # Set up threshold = 0.5\n",
    "    y_pred = [1 if (y >= 0.5).all() else 0 for y in y_pred]\n",
    "\n",
    "    # Model evaluation\n",
    "    print(color.BOLD_CYAN_COLOR + '\\n.:. Classification Report .:. '+ color.END)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(color.BOLD_GREEN_COLOR + '      Accuracy: %f' % accuracy + color.END,'\\n')\n",
    "    models['ANN'+'_'+data_type] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.720132Z",
     "start_time": "2022-12-03T08:23:19.718309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "ANN_model('ORI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.722810Z",
     "start_time": "2022-12-03T08:23:19.721219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "ANN_model('PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.725353Z",
     "start_time": "2022-12-03T08:23:19.723808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# High Correlation Filter\n",
    "ANN_model('CORR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:23:19.729386Z",
     "start_time": "2022-12-03T08:23:19.726593Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# return to dataframe\n",
    "models_df = pd.DataFrame(models.items())\n",
    "models_df.columns = ['model','accuracy']\n",
    "models_df['accuracy'] = round(models_df['accuracy']*100,3)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T16:21:47.485091Z",
     "start_time": "2022-12-04T16:21:46.859559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib==3.4.1 if has error in bar_label\n",
    "most_colors = ['#ED5564','#4FC1E8','#A0D568','#FFCE54','#AC92EB']\n",
    "model_lst = [[0,3],[3,6],[6,9],[9,12],[12,15]]\n",
    "plt.figure(figsize = (12, 5))\n",
    "sub_plot = 0\n",
    "\n",
    "for index,val in enumerate(model_lst):\n",
    "    ax = plt.subplot(1, 5, sub_plot+1)\n",
    "    df_model = models_df.iloc[val[0]:val[1],]\n",
    "    values = df_model['accuracy'].to_list()\n",
    "    clrs = ['#D3D3D3' if (x < max(values)) else most_colors[index] for x in values ]\n",
    "    plot = sns.barplot(data=df_model, x=\"model\", y=\"accuracy\",color=colors[1],palette=clrs)\n",
    "    plot.set(xticklabels=['ORI','PCA','CORR'])\n",
    "    plot.set(yticklabels=[])\n",
    "    plot.patch.set_edgecolor('black')  \n",
    "    plot.patch.set_linewidth('1')\n",
    "    plot.set(ylabel=None)\n",
    "    plot.bar_label(ax.containers[0], fmt='%.2f%%')\n",
    "    xlabel = ''.join(df_model['model'].to_list()[1].partition('_')[0:1])\n",
    "    plot.set_xlabel(xlabel, fontsize = 12, fontweight='heavy')\n",
    "    sub_plot += 1\n",
    "    plt.suptitle(f'Models Accuracy', fontweight='heavy',color=colors[0],fontsize=16) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "R9qkf9jbSMX-"
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D97271; color: #FFFFFF; padding: 12px; line-height: 1.5;\"> Dataset 2: Web Scrape of ScienceDirect </div>\n",
    "\n",
    "<div class=\"warning\" style='background-color:#f5dada; color: #000000; border-left: solid #d68181 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em'>\n",
    "<b>Workflow:</b></p>\n",
    "<p style='margin-left:1em;'>\n",
    "\n",
    "1. Web Scrap from Elsevier by using API key and under Institutional VPN\n",
    "2. Simple NLP implementation\n",
    "3. Results and Discussion\n",
    "\n",
    "</p>\n",
    "<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> <b></b> <i></i>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:47:37.143922Z",
     "start_time": "2022-12-03T11:47:37.136830Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\"> Web Scrape - Raw Data </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T06:20:53.433825Z",
     "start_time": "2022-12-04T06:20:53.430330Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API keys\n",
    "api1 = 'b0ae12317b37ed66678d9734039edf82'\n",
    "api2 = '3c8dc36b6e2bc0cdf8478beca1fbd040'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T06:20:54.595893Z",
     "start_time": "2022-12-04T06:20:54.559416Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def lst_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Class\n",
    "class ScienceDirectScrape():\n",
    "    \"\"\" Simple version of web scrape of ScienceDirect\n",
    "    \"\"\"\n",
    "    SEARCH = \"https://api.elsevier.com/content/search/\"\n",
    "    \n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        self._api_key = '' #Ciphertext storage\n",
    "        self.search_type = 'sciencedirect'\n",
    "        self.search_url = self.SEARCH + self.search_type + '?query=' + urllib.parse.quote(self.query)\n",
    "        self.df1 = pd.DataFrame()\n",
    "        self.df2 = pd.DataFrame()\n",
    "\n",
    "    @property\n",
    "    def api_key(self):\n",
    "        return self._api_key\n",
    "    @api_key.setter\n",
    "    def api_key(self, api_key):\n",
    "        if api_key == None or len(api_key) != 32:\n",
    "            raise ValueError('Invalid API Key. Please Check in Elsevier Developer Portal.')\n",
    "        self._api_key = api_key\n",
    "    \n",
    "    @property\n",
    "    def total_result(self):\n",
    "        return self._total_result\n",
    "    \n",
    "    def start_search(self):\n",
    "        \"\"\"Wait for edit\n",
    "        \"\"\"\n",
    "        print(color.BOLD_GREEN_COLOR + '\\n Start to scrape the results:'+ color.END)\n",
    "        # Initilization\n",
    "        headers={\"X-ELS-APIKey\":self.api_key,\"Accept\":'application/json'}\n",
    "        timeout = httpx.Timeout(10.0, connect = 30.0)\n",
    "        client = httpx.Client(timeout = timeout,headers = headers)\n",
    "        url = self.search_url\n",
    "        \n",
    "        # Step 1\n",
    "        response = client.get(url)\n",
    "        doc = json.loads(response.text)\n",
    "        print(response)\n",
    "        self._total_result = int(doc['search-results']['opensearch:totalResults'])\n",
    "        index = len(doc['search-results']['entry'])\n",
    "        \n",
    "        pbar = tqdm(initial = 25, total = self.total_result)\n",
    "        while index <= self.total_result:\n",
    "            results = doc['search-results']['entry']\n",
    "            \n",
    "            # get doi\n",
    "            try:\n",
    "                for r in results:\n",
    "                    self.df1 = self.df1.append({'doi': r['prism:doi']}, ignore_index = True)\n",
    "                 # get next page url\n",
    "                for l in doc['search-results']['link']:\n",
    "                    if l['@ref'] == 'next':\n",
    "                        url = l['@href']\n",
    "\n",
    "                # connect to next page\n",
    "                time.sleep(1.25)\n",
    "                response = client.get(url)\n",
    "                doc = json.loads(response.text)\n",
    "                index += len(doc['search-results']['entry'])\n",
    "                pbar.update(len(doc['search-results']['entry']))\n",
    "            except:\n",
    "                continue\n",
    "              \n",
    "        print(color.BOLD_RED_COLOR + '\\n Complete to scrape the results'+ color.END)\n",
    "        pbar.close()\n",
    "\n",
    "    def start_scrape_doi(self,doi_lst,index):\n",
    "        \"\"\"wait for edit\n",
    "        \"\"\"\n",
    "        print(color.BOLD_GREEN_COLOR + f'\\n Start to Find keywords and Abstracts - {index}:'+ color.END)\n",
    "        \n",
    "        # Initilization\n",
    "        headers={\"X-ELS-APIKey\":self.api_key,\"Accept\":'application/json'}\n",
    "        timeout = httpx.Timeout(10.0, connect=120.00)\n",
    "        client = httpx.Client(timeout=timeout,headers=headers)\n",
    "    \n",
    "        for i in tqdm(range(len(doi_lst))):\n",
    "            url = f\"https://api.elsevier.com/content/article/doi/\"+ doi_lst[i]\n",
    "            time.sleep(1.25)\n",
    "            try:\n",
    "                response = client.get(url)\n",
    "                doc = json.loads(response.text)\n",
    "                self.df2 = self.df2.append({'title': doc['full-text-retrieval-response']['coredata']['dc:title'],\n",
    "                            'author': [x['$'] for x in doc['full-text-retrieval-response']['coredata']['dc:creator']],\n",
    "                            'publish_date': doc['full-text-retrieval-response']['coredata']['prism:coverDate'],\n",
    "                            'keywords': [x['$'] for x in doc['full-text-retrieval-response']['coredata']['dcterms:subject']],\n",
    "                            'abstract': doc['full-text-retrieval-response']['coredata']['dc:description'],\n",
    "                            'link': doc['full-text-retrieval-response']['coredata']['link'][1]['@href']},\n",
    "                             ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "        print(color.BOLD_RED_COLOR + f'Completed - {index}\\n'+ color.END)\n",
    "    \n",
    "    def extract_doi(self):\n",
    "        \"\"\"200 doi per times\n",
    "        \"\"\"\n",
    "        doi_lst = self.df1['doi'].to_list()\n",
    "        doi_lst_chunks = list(lst_chunks(doi_lst,200))\n",
    "        print(color.BOLD + f' Total: {len(doi_lst_chunks)} times'+ color.END)\n",
    "        for index,lst in enumerate(doi_lst_chunks):\n",
    "            self.start_scrape_doi(lst,index)\n",
    "            print('     Time sleep for 5 seconds....')\n",
    "            time.sleep(5)\n",
    "        print(color.BOLD_RED_COLOR + f'Completed\\n'+ color.END) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T06:23:10.514126Z",
     "start_time": "2022-12-04T06:20:57.133604Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Web scrape\n",
    "search = ScienceDirectScrape('wisconsin-breast-cancer-machine-learning-model')\n",
    "search.api_key = api2\n",
    "search.start_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T07:30:27.645116Z",
     "start_time": "2022-12-04T06:23:10.575895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search.api_key = api1\n",
    "search.extract_doi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T07:36:31.562598Z",
     "start_time": "2022-12-04T07:36:31.225217Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert to xlsx file\n",
    "df = search.df2\n",
    "df.to_excel(\"output.xlsx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\"> Simple NLP Implementation</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set up\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Code reference: \n",
    " - https://www.kaggle.com/code/ananyaroy1011/automated-keyword-extraction-from-articles-nlp\n",
    "\"\"\"\n",
    "\n",
    "def corpus(df):\n",
    "    lst= []\n",
    "    for i in range(0, int(df.word_count.describe()['count'])):\n",
    "        text = re.sub('[^a-zA-Z]', ' ', df['abstract'].to_list()[i])\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        text = text.split()\n",
    "\n",
    "        # Stemming\n",
    "        ps = PorterStemmer()\n",
    "        # Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n",
    "        text = \" \".join(text)\n",
    "        lst.append(text)\n",
    "    return corpus\n",
    "\n",
    "# Frequently occuring words function\n",
    "def freq_words(corpuslst, n=20, num_words = 1, figsize = (13,8)):\n",
    "    \n",
    "    if num_words == 1:\n",
    "        vec = CountVectorizer().fit(corpuslst)\n",
    "    else:\n",
    "        vec = CountVectorizer(ngram_range=(num_words,num_words), max_features=2000).fit(corpuslst)\n",
    "\n",
    "    # bag of words model\n",
    "    bag_of_words = vec.transform(corpuslst)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "\n",
    "    # words_freq\n",
    "    freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n",
    "    top_words = freq[:n]\n",
    "    display(top_words)\n",
    "    print('\\n')\n",
    "\n",
    "    # graph\n",
    "    top_df = pd.DataFrame(top_words)\n",
    "    top_df.columns=[\"Word\", \"Freq\"]\n",
    "    sns.set(rc={'figure.figsize':figsize})\n",
    "    g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our raw data after doing web scraping by using keyword `wisconsin breast cancer machine learning model` is ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Screening\n",
    "#### Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count each abstract's word counts\n",
    "df['word_count'] = df['abstract'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['abstract','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop row if some paper has wrong format of abstracts\n",
    "for index,val in enumerate(df['abstract'].to_list()):\n",
    "    if type(val) != str:\n",
    "        df = df.drop(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To avoid bias and we applly strict rules in selecting if the reasearch paper is relative about building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualization of top n words for the abstracts\n",
    "corpus_filter = corpus(df)\n",
    "freq_words(corpus_filter, n=20, num_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_words(corpus_filter, n=20, num_words=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_words(corpus_filter, n=20, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first filter\n",
    "new_df = pd.DataFrame()\n",
    "for index,val in enumerate(df['abstract'].to_list()):\n",
    "    try:\n",
    "        if 'deep learning' in val.lower() or 'machine learning' in val.lower() or 'machine learning algorithm' in val.lower():\n",
    "        new_df = new_df.append(df.iloc[index,:])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Second filter\n",
    "new_df2 = pd.DataFrame()\n",
    "for index,val in enumerate(new_df['abstract'].to_list()):\n",
    "    try:\n",
    "        if 'accuracy' in val.lower() or 'performance' in val.lower() or 'predict' in val.lower or 'classifier' in val.lower():\n",
    "            new_df2 = new_df2.append(new_df.iloc[index,:])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualization of top n words for the abstracts\n",
    "corpus_filter = corpus(new_df2)\n",
    "freq_words(corpus_filter, n=20, num_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_words(corpus_filter, n=20, num_words=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq_words(corpus_filter, n=20, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Numbers of published paper by year\n",
    "new_df2['publish_date'] = pd.to_datetime(new_df2['publish_date'])\n",
    "new_df2['publish_date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some irrelavant papers are screened out by using abstracts. In the next step we will use keywords as the second screening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# return keywords as a list\n",
    "t = []\n",
    "for i in new_df2['keywords'].to_list():\n",
    "    t.extend(ast.literal_eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# keywords preprocessing\n",
    "import copy\n",
    "t1 = t.copy()\n",
    "\n",
    "def check(s):\n",
    "    regex = re.compile(r'\\((\\w+)\\)')\n",
    "    return bool(regex.search(s))\n",
    "\n",
    "for i in t:\n",
    "    if check(i):\n",
    "        t1.insert(0, t1.pop(t1.index(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T05:36:49.271909Z",
     "start_time": "2022-12-04T05:36:49.260791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# abbre\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe('abbreviation_detector')\n",
    "\n",
    "def replace_acronyms(text):\n",
    "    doc = nlp(text)\n",
    "    altered_tok = [tok.text for tok in doc]\n",
    "    for abrv in doc._.abbreviations:\n",
    "        altered_tok[abrv.start] = str(abrv._.long_form)\n",
    "\n",
    "    return(\" \".join(altered_tok))\n",
    "\n",
    "def convert_into_uppercase(a):\n",
    "    return a.group(1) + a.group(2).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T05:40:20.991355Z",
     "start_time": "2022-12-04T05:40:20.084469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t2 = [re.sub(\"(^|\\s)(\\S)\", convert_into_uppercase, i) for i in t1]\n",
    "t3 = ', '.join(t2)\n",
    "t3 = t3.replace('+','')\n",
    "t4 = replace_acronyms(t3).split(\", \")\n",
    "\n",
    "for index,val in enumerate(t4):\n",
    "    result = re.sub(r'\\([^)]*\\)', '', val)\n",
    "    head, sep, tail = result.partition('(')\n",
    "    t4[index] = head[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in t4:\n",
    "    i = re.sub(\"(\\\\d|\\\\W)+\",\" \", i)\n",
    "    i = i.lower()\n",
    "    i = replace_acronyms(i).split(\" \")\n",
    "\n",
    "    for index,s in enumerate(i):\n",
    "        i[index] = nltk.WordNetLemmatizer().lemmatize(s)\n",
    "\n",
    "    i = ' '.join(i)\n",
    "    lst.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## <div style=\"font-family: Trebuchet MS; background-color: #f1acab; color: #FFFFFF; padding: 12px; line-height: 1.5; border-left: solid #d68181 4px; border-radius: 5px\"> Results and Discussion </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "RlRDNfv4c8I1"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Code References:**\n",
    "1. []()\n",
    "\n",
    "**Literatures:**\n",
    "1. []()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
